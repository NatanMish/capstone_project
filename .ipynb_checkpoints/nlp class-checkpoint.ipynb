{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp:\n",
    "    #need to download from nltk - 'averaged_perceptron_tagger','wordnet','stopwords'\n",
    "    re = __import__('re')\n",
    "    pd = __import__('pandas')\n",
    "    nltk = __import__('nltk')\n",
    "    np = __import__('numpy')\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('nlp V.0.1 \\nImported pandas,re,nltk,numpy packages')\n",
    "        \n",
    "    def clean_text_for_topic_modelling(self,text_column):\n",
    "        import string\n",
    "        text_column=text_column.str.lower()\n",
    "        text_column = [self.re.sub(r'https?:\\/\\/*[^ ]*', '', x) for x in text_column]\n",
    "#         text_column = [self.re.sub(r'[.,;/]',' ', x) for x in text_column]\n",
    "        text_column = [self.re.sub(r'\\((cont)\\)','', x) for x in text_column]\n",
    "        text_column = [self.re.sub(r'[^A-Za-z0-9$% ]','', x) for x in text_column]\n",
    "        text_column = [x.translate(str.maketrans('', '', string.punctuation)) for x in text_column]\n",
    "        text_column = [x.split() for x in text_column]\n",
    "        temp_corpus=[]\n",
    "        for tweet in text_column:\n",
    "            new_list=[w for w in tweet if len(w)>2]\n",
    "            temp_corpus.append(new_list)\n",
    "        text_column=self.pd.Series(temp_corpus)\n",
    "        return text_column\n",
    "    \n",
    "    def remove_stopwords_from_corpus(self,text_column,extra_stopwords_list=[]):\n",
    "        from nltk.corpus import stopwords\n",
    "        stops = set(stopwords.words('english')).union(extra_stopwords_list)\n",
    "        text_column=text_column.apply(lambda x: [w for w in x if w not in stops])\n",
    "        return text_column\n",
    "    \n",
    "    def stemm_corpus(self,text_column,stemmer='porter'):\n",
    "        if stemmer=='porter':\n",
    "            from nltk.stem import PorterStemmer\n",
    "            porter = PorterStemmer()\n",
    "            text_column=text_column.apply(lambda x:[porter.stem(w) for w in x])\n",
    "        elif stemmer=='lancaster':\n",
    "            from nltk.stem import LancasterStemmer\n",
    "            lancaster=LancasterStemmer()\n",
    "            text_column=text_column.apply(lambda x:[lancaster.stem(w) for w in x])\n",
    "        else:\n",
    "            from nltk.stem import SnowballStemmer\n",
    "            snowball=SnowballStemmer(\"english\")\n",
    "            text_column=text_column.apply(lambda x:[snowball.stem(w) for w in x])\n",
    "        return text_column\n",
    "    \n",
    "    def lemmatize_corpus(self,text_column):\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        from nltk.corpus import wordnet\n",
    "        \n",
    "        def get_word_pos(word):\n",
    "            tag = self.nltk.pos_tag(word)[0][1][0].upper()\n",
    "            tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "            return tag_dict.get(tag, wordnet.NOUN)\n",
    "        \n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        text_column=text_column.apply(lambda x:[wordnet_lemmatizer.lemmatize(w, pos=get_word_pos(w)) for w in x])\n",
    "        return text_column\n",
    "    \n",
    "    def create_ngrams(self,text_column,replace=False,num_grams=2):\n",
    "        from nltk import ngrams\n",
    "        if replace:\n",
    "            grams_list=[]\n",
    "            for tweet in text_column:\n",
    "                grams_list.append([' '.join(ngram) for ngram in ngrams(tweet,num_grams)])\n",
    "            return self.pd.Series(grams_list)\n",
    "        else:\n",
    "            for i,tweet in text_column.iteritems():\n",
    "                copy_tweet=tweet.copy()\n",
    "                for ngram in ngrams(tweet,2):\n",
    "                    copy_tweet.append(' '.join(ngram))\n",
    "                text_column.at[i]=copy_tweet\n",
    "            return text_column\n",
    "        \n",
    "    def gensim_dic(self,text_column,filter_extremes=True,no_below=5,no_above=0.85):\n",
    "        from gensim.corpora import Dictionary\n",
    "        dictionary=Dictionary(text_column)\n",
    "        if filter_extremes:\n",
    "            dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "        return dictionary\n",
    "    \n",
    "    def bow_corpus(self,dictionary,text_column):\n",
    "        return [dictionary.doc2bow(doc) for doc in text_column]\n",
    "    \n",
    "    def one_hot_encoding_todf(self,dictionary,bow_corp):\n",
    "        import gensim\n",
    "        return self.pd.DataFrame(gensim.matutils.corpus2dense(bow_corp,num_terms=len(dictionary))).T\n",
    "    \n",
    "    def tf_idf_tomatrix(self,dictionary,tf_idf_object):\n",
    "        from gensim.matutils import corpus2dense\n",
    "        num_terms = len(dictionary.keys())\n",
    "        num_docs = dictionary.num_docs\n",
    "        corpus_tfidf_dense = corpus2dense(tf_idf_object, num_terms, num_docs)\n",
    "        return self.pd.DataFrame(self.np.transpose(corpus_tfidf_dense))\n",
    "        \n",
    "    def vec_to_tfidf(self,bow_corpus):\n",
    "        from gensim import models\n",
    "        tfidf = models.TfidfModel(bow_corpus)\n",
    "        return tfidf[bow_corpus]\n",
    "    \n",
    "    def lda_model(self,tfidf_corp,dic,num_topics=20,passes=30,alpha=0.001,eta='auto'):\n",
    "        from gensim import models\n",
    "        return models.ldamodel.LdaModel(corpus=tfidf_corp, num_topics=num_topics, id2word=dic, passes=passes, alpha=alpha, \n",
    "                                             eta=eta,random_state=13)\n",
    "    \n",
    "    def get_coherence(self,lda_model,corpus,dic):\n",
    "        from gensim.models import CoherenceModel\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus, dictionary=dic, coherence='c_v')\n",
    "        return coherence_model_lda.get_coherence()\n",
    "    \n",
    "    def assign_lda_topic(self,lda_model,bow_corpus):\n",
    "        topics=[]\n",
    "        data=lda_model.get_document_topics(bow_corpus)\n",
    "        for tweet in data:  \n",
    "            df = self.pd.DataFrame(tweet, columns=['topic_num', 'probability'])\n",
    "            topics.append(df.loc[df['probability'].idxmax(), 'topic_num'])\n",
    "        return (self.pd.Series(topics))\n",
    "    \n",
    "    def get_sentiment(self,text_column):\n",
    "        from textblob import TextBlob\n",
    "        return text_column.apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    \n",
    "    def get_subjectivity(self,text_column):\n",
    "        from textblob import TextBlob\n",
    "        return text_column.apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    \n",
    "    def get_length_of_comment(self,text_column):\n",
    "        return text_column.apply(lambda x: len(x))\n",
    "    \n",
    "    def amount_of_upper(self,text_column):\n",
    "        return text_column.apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    \n",
    "    def create_uppercase_max_sequence_column(self,text_column):\n",
    "        def get_max_uppercase_run_from_string(s):\n",
    "            lengths=[len(x) for x in self.re.findall(r\"[A-Z]+\", s)]\n",
    "            if len(lengths)>0:\n",
    "                return max(lengths)\n",
    "            else:\n",
    "                return 0\n",
    "        return (text_column.apply(lambda x: get_max_uppercase_run_from_string(x)))\n",
    "    \n",
    "    def amount_of_sign(self,text_column,sign):\n",
    "        return text_column.apply(lambda x: sum(1 for c in x if c==sign))\n",
    "    \n",
    "    def max_sequence(self,text_column,symbol):\n",
    "        def get_max_sequence_run_from_string(s,symbol):\n",
    "            lengths=[len(x) for x in self.re.findall(r\"[\"+symbol+\"]+\", s)]\n",
    "            if len(lengths)>0:\n",
    "                return max(lengths)\n",
    "            else:\n",
    "                return 0   \n",
    "        return text_column.apply(lambda x: get_max_sequence_run_from_string(x,symbol))\n",
    "    \n",
    "    def bad_comments_column_bin(self,text_column,bad_words_list):\n",
    "        def is_comment_bad(comment,bad_words_list):\n",
    "            for word in comment:\n",
    "                if word in bad_words_list:\n",
    "                    return 1\n",
    "            return 0\n",
    "        return text_column.apply(lambda x: is_comment_bad(x,bad_words_list))\n",
    "\n",
    "    def bad_comments_column_agg(self,text_column,bad_words_list):\n",
    "        def amount_comment_bad_words(comment,bad_words_list):\n",
    "            count_of_bad_words=0\n",
    "            for word in comment:\n",
    "                if word in bad_words_list:\n",
    "                    count_of_bad_words=count_of_bad_words+1\n",
    "            return count_of_bad_words\n",
    "        return text_column.apply(lambda x: amount_comment_bad_words(x,bad_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('trump_tweets_reducted.csv',parse_dates=['created_at_utc'],low_memory=False)\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp V.0.1 \n",
      "Imported pandas,re,nltk,numpy packages\n"
     ]
    }
   ],
   "source": [
    "nlpob=nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=nlpob.clean_text_for_topic_modelling(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=nlpob.remove_stopwords_from_corpus(corpus,['realdonaldtrump','amp','president','android','iphone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_without_stemorlem=corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus=nlpob.stemm_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus=nlpob.lemmatize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=nlpob.create_ngrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_words=['china','chinese','nafta','trade','trades','trading','tariff','tariffs','opec','usmca','xi','jinping','sanctions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trade_topic']=nlpob.bad_comments_column_agg(corpus,trade_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_words=['market','markets','stock','stocks','financial','investment','dow','nasdaq','500','wall street','wall st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['financial_topic']=nlpob.bad_comments_column_agg(corpus,financial_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labor_words=['unemployment','jobs','labor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labor_topic']=nlpob.bad_comments_column_agg(corpus,labor_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_words=['manufacture','manufacturers','consumer','consumers','bank','banks','factories','business','businesses'\n",
    "               ,'corporate','corporates','industry','industries','product','agriculture','agricultural','products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['industry_topic']=nlpob.bad_comments_column_agg(corpus,industry_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_rates_topics=['rate','rates','reserve','inflation','currency','depreciating','depreceate','fed','federal reserve']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['currency_rates_topic']=nlpob.bad_comments_column_agg(corpus,currency_rates_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words=['deal','deals',\n",
    "             'dollar','dollars','$',\n",
    "             'billion','billions','gdp','growth',\n",
    "             'revenue','economy','economies','economist','economic','economists','money',\n",
    "             'companies',\n",
    "             'price','prices',\n",
    "             'cents','cent','purchase',\n",
    "             'depletion','regulation',\n",
    "             '401(k)','trillions','recession','depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['general_related_topic']=nlpob.bad_comments_column_agg(corpus,topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_words=['taxes','taxation','tax','debt','deficit','spending','refinance','finance','savings','deficits','bankruptcy',\n",
    "              'spend','cost','costs','subsidizing','subsidize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['budget_topic']=nlpob.bad_comments_column_agg(corpus,budget_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_words=['iran','nuclear','wall','military','daca','bill','danger','conflict','rockets','russia',\n",
    "                 'middle east','ukrainian','ukraine','isis','syria','border','russian','investigation','kim','jong','un',\n",
    "                'caravan','sanctions','corona','virus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sensitive_topic']=nlpob.bad_comments_column_agg(corpus,sensitive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic=nlpob.gensim_dic(corpus)\n",
    "# bow_corp=nlpob.bow_corpus(dic,corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_corp=nlpob.vec_to_tfidf(bow_corp)\n",
    "# lda_model = nlpob.lda_model(tfidf_corp,dic)\n",
    "# nlpob.get_coherence(lda_model,corpus,dic)\n",
    "# df['lda_topic'] = nlpob.assign_lda_topic(lda_model,bow_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot=nlpob.one_hot_encoding_todf(dic,bow_corp)\n",
    "# one_hot.columns=[dic.get(x) for x in range(len(dic))]\n",
    "# df=df.join(one_hot,rsuffix='onehotencoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = nlpob.get_sentiment(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subjectivity'] = nlpob.get_subjectivity(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('trump_tweets_reducted_after_nlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,x in df[['sentiment','text']].iterrows():\n",
    "#     print(x['sentiment'])\n",
    "#     print(x['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for idx, topic in lda_model.print_topics(-1):\n",
    "# #     print('Topic: {} Word: {}'.format(idx, topic))\n",
    "\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim \n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, bow_corp, dic)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hdpmodel\n",
    "# from gensim.corpora import Dictionary\n",
    "# dct = Dictionary(corpus)\n",
    "# vector_corpus=[dct.doc2bow(tweet) for tweet in corpus]\n",
    "# from gensim.models import HdpModel\n",
    "# hdp = HdpModel(vector_corpus, dct,alpha=0.001)\n",
    "# topic_info = hdp.print_topics(num_topics=10, num_words=20)\n",
    "# topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare=pd.DataFrame({'corpus':corpus,'real':df['text']})\n",
    "# for i,row in compare[17000:].iterrows():\n",
    "#     print(row['corpus'])\n",
    "#     print('\\n')\n",
    "#     print(row['real'])\n",
    "#     time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
