{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp:\n",
    "    #need to download from nltk - 'averaged_perceptron_tagger','wordnet','stopwords'\n",
    "    re = __import__('re')\n",
    "    pd = __import__('pandas')\n",
    "    nltk = __import__('nltk')\n",
    "    np = __import__('numpy')\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('nlp V.0.1 \\nImported pandas,re,nltk,numpy packages')\n",
    "        \n",
    "    def clean_text_for_topic_modelling(self,text_column):\n",
    "        import string\n",
    "        text_column=text_column.str.lower()\n",
    "        text_column = [self.re.sub(r'https?:\\/\\/*[^ ]*', '', x) for x in text_column]\n",
    "#         text_column = [self.re.sub(r'[.,;/]',' ', x) for x in text_column]\n",
    "        text_column = [self.re.sub(r'\\((cont)\\)','', x) for x in text_column]\n",
    "        text_column = [self.re.sub(r'[^A-Za-z0-9$% ]','', x) for x in text_column]\n",
    "        text_column = [x.translate(str.maketrans('', '', string.punctuation)) for x in text_column]\n",
    "        text_column = [x.split() for x in text_column]\n",
    "        temp_corpus=[]\n",
    "        for tweet in text_column:\n",
    "            new_list=[w for w in tweet if len(w)>2]\n",
    "            temp_corpus.append(new_list)\n",
    "        text_column=self.pd.Series(temp_corpus)\n",
    "        return text_column\n",
    "    \n",
    "    def remove_stopwords_from_corpus(self,text_column,extra_stopwords_list=[]):\n",
    "        from nltk.corpus import stopwords\n",
    "        stops = set(stopwords.words('english')).union(extra_stopwords_list)\n",
    "        text_column=text_column.apply(lambda x: [w for w in x if w not in stops])\n",
    "        return text_column\n",
    "    \n",
    "    def stemm_corpus(self,text_column,stemmer='porter'):\n",
    "        if stemmer=='porter':\n",
    "            from nltk.stem import PorterStemmer\n",
    "            porter = PorterStemmer()\n",
    "            text_column=text_column.apply(lambda x:[porter.stem(w) for w in x])\n",
    "        elif stemmer=='lancaster':\n",
    "            from nltk.stem import LancasterStemmer\n",
    "            lancaster=LancasterStemmer()\n",
    "            text_column=text_column.apply(lambda x:[lancaster.stem(w) for w in x])\n",
    "        else:\n",
    "            from nltk.stem import SnowballStemmer\n",
    "            snowball=SnowballStemmer(\"english\")\n",
    "            text_column=text_column.apply(lambda x:[snowball.stem(w) for w in x])\n",
    "        return text_column\n",
    "    \n",
    "    def lemmatize_corpus(self,text_column):\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        from nltk.corpus import wordnet\n",
    "        \n",
    "        def get_word_pos(word):\n",
    "            tag = self.nltk.pos_tag(word)[0][1][0].upper()\n",
    "            tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "            return tag_dict.get(tag, wordnet.NOUN)\n",
    "        \n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        text_column=text_column.apply(lambda x:[wordnet_lemmatizer.lemmatize(w, pos=get_word_pos(w)) for w in x])\n",
    "        return text_column\n",
    "    \n",
    "    def create_ngrams(self,text_column,replace=False,num_grams=2):\n",
    "        from nltk import ngrams\n",
    "        if replace:\n",
    "            grams_list=[]\n",
    "            for tweet in text_column:\n",
    "                grams_list.append([' '.join(ngram) for ngram in ngrams(tweet,num_grams)])\n",
    "            return self.pd.Series(grams_list)\n",
    "        else:\n",
    "            for i,tweet in text_column.iteritems():\n",
    "                copy_tweet=tweet.copy()\n",
    "                for ngram in ngrams(tweet,2):\n",
    "                    copy_tweet.append(' '.join(ngram))\n",
    "                text_column.at[i]=copy_tweet\n",
    "            return text_column\n",
    "        \n",
    "    def gensim_dic(self,text_column,filter_extremes=True,no_below=5,no_above=0.85):\n",
    "        from gensim.corpora import Dictionary\n",
    "        dictionary=Dictionary(text_column)\n",
    "        if filter_extremes:\n",
    "            dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "        return dictionary\n",
    "    \n",
    "    def bow_corpus(self,dictionary,text_column):\n",
    "        return [dictionary.doc2bow(doc) for doc in text_column]\n",
    "    \n",
    "    def one_hot_encoding_todf(self,dictionary,bow_corp):\n",
    "        import gensim\n",
    "        return self.pd.DataFrame(gensim.matutils.corpus2dense(bow_corp,num_terms=len(dictionary))).T\n",
    "    \n",
    "    def tf_idf_tomatrix(self,dictionary,tf_idf_object):\n",
    "        from gensim.matutils import corpus2dense\n",
    "        num_terms = len(dictionary.keys())\n",
    "        num_docs = dictionary.num_docs\n",
    "        corpus_tfidf_dense = corpus2dense(tf_idf_object, num_terms, num_docs)\n",
    "        return self.pd.DataFrame(self.np.transpose(corpus_tfidf_dense))\n",
    "        \n",
    "    def vec_to_tfidf(self,bow_corpus):\n",
    "        from gensim import models\n",
    "        tfidf = models.TfidfModel(bow_corpus)\n",
    "        return tfidf[bow_corpus]\n",
    "    \n",
    "    def lda_model(self,tfidf_corp,dic,num_topics=20,passes=30,alpha=0.001,eta='auto'):\n",
    "        from gensim import models\n",
    "        return models.ldamodel.LdaModel(corpus=tfidf_corp, num_topics=num_topics, id2word=dic, passes=passes, alpha=alpha, \n",
    "                                             eta=eta,random_state=13)\n",
    "    \n",
    "    def get_coherence(self,lda_model,corpus,dic):\n",
    "        from gensim.models import CoherenceModel\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus, dictionary=dic, coherence='c_v')\n",
    "        return coherence_model_lda.get_coherence()\n",
    "    \n",
    "    def assign_lda_topic(self,lda_model,bow_corpus):\n",
    "        topics=[]\n",
    "        data=lda_model.get_document_topics(bow_corpus)\n",
    "        for tweet in data:  \n",
    "            df = self.pd.DataFrame(tweet, columns=['topic_num', 'probability'])\n",
    "            topics.append(df.loc[df['probability'].idxmax(), 'topic_num'])\n",
    "        return (self.pd.Series(topics))\n",
    "    \n",
    "    def get_sentiment(self,text_column):\n",
    "        from textblob import TextBlob\n",
    "        return text_column.apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    \n",
    "    def get_subjectivity(self,text_column):\n",
    "        from textblob import TextBlob\n",
    "        return text_column.apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    \n",
    "    def get_length_of_comment(self,text_column):\n",
    "        return text_column.apply(lambda x: len(x))\n",
    "    \n",
    "    def amount_of_upper(self,text_column):\n",
    "        return text_column.apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    \n",
    "    def create_uppercase_max_sequence_column(self,text_column):\n",
    "        def get_max_uppercase_run_from_string(s):\n",
    "            lengths=[len(x) for x in self.re.findall(r\"[A-Z]+\", s)]\n",
    "            if len(lengths)>0:\n",
    "                return max(lengths)\n",
    "            else:\n",
    "                return 0\n",
    "        return (text_column.apply(lambda x: get_max_uppercase_run_from_string(x)))\n",
    "    \n",
    "    def amount_of_sign(self,text_column,sign):\n",
    "        return text_column.apply(lambda x: sum(1 for c in x if c==sign))\n",
    "    \n",
    "    def max_sequence(self,text_column,symbol):\n",
    "        def get_max_sequence_run_from_string(s,symbol):\n",
    "            lengths=[len(x) for x in self.re.findall(r\"[\"+symbol+\"]+\", s)]\n",
    "            if len(lengths)>0:\n",
    "                return max(lengths)\n",
    "            else:\n",
    "                return 0   \n",
    "        return text_column.apply(lambda x: get_max_sequence_run_from_string(x,symbol))\n",
    "    \n",
    "    def bad_comments_column_bin(self,text_column,bad_words_list):\n",
    "        def is_comment_bad(comment,bad_words_list):\n",
    "            for word in comment:\n",
    "                if word in bad_words_list:\n",
    "                    return 1\n",
    "            return 0\n",
    "        return text_column.apply(lambda x: is_comment_bad(x,bad_words_list))\n",
    "\n",
    "    def bad_comments_column_agg(self,text_column,bad_words_list):\n",
    "        def amount_comment_bad_words(comment,bad_words_list):\n",
    "            count_of_bad_words=0\n",
    "            if len(comment)>0:\n",
    "                for word in comment:\n",
    "                    if word in bad_words_list:\n",
    "                        count_of_bad_words=count_of_bad_words+1\n",
    "                return count_of_bad_words\n",
    "            return 0\n",
    "        return text_column.apply(lambda x: amount_comment_bad_words(x,bad_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./vp_tweets/vp.csv',parse_dates=['created_at'],low_memory=False)\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp V.0.1 \n",
      "Imported pandas,re,nltk,numpy packages\n"
     ]
    }
   ],
   "source": [
    "nlpob=nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=nlpob.clean_text_for_topic_modelling(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus=nlpob.remove_stopwords_from_corpus(corpus,['realdonaldtrump','amp','president','android','iphone'])\n",
    "corpus=nlpob.remove_stopwords_from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another try for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df = 20)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=number_topics,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(X)\n",
    "lda_results = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.score(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results = pd.DataFrame(lda_results, columns = [f'topic_{i}' for i in range(0,50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,lda_results], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toinspect = pd.DataFrame(lda.components_,columns = vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toinspect = pd.DataFrame(df_toinspect.idxmax(axis=1), columns=['topic']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn LDA viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pyLDAvis\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, X, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=nlpob.create_ngrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_words =  ['nafta', 'trade', 'trades', 'trading', 'tariff', 'tariffs', 'opec', 'usmca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trade_topic'] = nlpob.bad_comments_column_agg(corpus,trade_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_words = ['china', 'chinese', 'xi', 'jinping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['china_topic'] = nlpob.bad_comments_column_agg(corpus,china_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_words = ['market', 'markets', 'stock', 'stocks', 'financial', 'investment', 'dow', 'nasdaq', '500', 'wall street',\n",
    "                   'wall st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['financial_topic'] = nlpob.bad_comments_column_agg(corpus,financial_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labor_words = ['unemployment','jobs', 'labor','employment', 'work', 'workers', 'payroll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labor_topic'] = nlpob.bad_comments_column_agg(corpus,labor_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_words= ['manufacture','manufacturers', 'consumer', 'consumers', 'bank', 'banks', 'factories', 'business',\n",
    "                 'businesses' ,'corporate', 'corporates', 'industry', 'industries', 'product', 'agriculture', \n",
    "                 'agricultural', 'products', 'companies', 'production', 'competitive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['industry_topic'] = nlpob.bad_comments_column_agg(corpus,industry_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_rates_topics = ['rate', 'rates', 'reserve', 'inflation', 'currency', 'depreciating', 'depreceate', 'fed', \n",
    "                         'federal reserve', 'powell', 'stimulate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['currency_rates_topic'] = nlpob.bad_comments_column_agg(corpus,currency_rates_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words=['deal', 'deals', 'dollar', 'dollars', '$', 'billion', 'billions', 'gdp', 'growth', 'revenue', 'economy',\n",
    "             'economies', 'economist', 'economic', 'economists', 'money', 'price', 'prices', 'cents', 'cent', 'purchase',\n",
    "             'depletion', 'regulation', '401(k)', 'trillions', 'recession', 'bill', 'military']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['general_related_topic'] = nlpob.bad_comments_column_agg(corpus,topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_words=['taxes', 'taxation', 'tax', 'debt', 'deficit', 'spending', 'refinance', 'finance', 'savings', 'deficits',\n",
    "              'bankruptcy', 'spend', 'cost', 'costs', 'subsidizing', 'subsidize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['budget_topic'] = nlpob.bad_comments_column_agg(corpus,budget_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreign_policy_words=['sanctions', 'iran', 'nuclear', 'wall', 'mexico', 'daca', 'conflict', 'rockets', 'russia',\n",
    "                      'middle east', 'ukrainian', 'ukraine', 'isis', 'syria', 'border', 'russian', 'iraq', 'kim', \n",
    "                      'jong', 'un', 'caravan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['foreign_policy_topic'] = nlpob.bad_comments_column_agg(corpus,foreign_policy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_words=['corona', 'virus', 'covid', 'chinese virus', 'containment', 'ventilators', 'h1n1',\n",
    "             'swine', 'flu', 'pandemic', 'coronavirus', 'killthevirus', 'masks', 'quarantined', 'quarantine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['covid_topic']=nlpob.bad_comments_column_agg(corpus,covid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_words=['danger', 'investigation', 'shutdown', 'crisis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sensitive_topic']=nlpob.bad_comments_column_agg(corpus,sensitive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_words=['coal', 'global warming', 'oil', 'wind', 'greta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['environment_topic']=nlpob.bad_comments_column_agg(corpus,environment_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = nlpob.get_sentiment(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['sentiment'] > 0, 'sentiment'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['sentiment'] < 0, 'sentiment'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5657 entries, 0 to 5656\n",
      "Data columns (total 37 columns):\n",
      "str_id                   5657 non-null int64\n",
      "screen_name              5657 non-null object\n",
      "utc_date                 5657 non-null object\n",
      "created_at               5657 non-null datetime64[ns]\n",
      "source                   5657 non-null object\n",
      "favorite_count           5657 non-null int64\n",
      "retweet_count            5657 non-null int64\n",
      "replies                  5657 non-null int64\n",
      "quotes                   5657 non-null int64\n",
      "lang                     5657 non-null object\n",
      "tweet_type               5657 non-null object\n",
      "text                     5657 non-null object\n",
      "quote                    615 non-null object\n",
      "country_code             2 non-null object\n",
      "place                    2 non-null object\n",
      "latitude                 2 non-null float64\n",
      "longitude                2 non-null float64\n",
      "is_specific_geo          5657 non-null object\n",
      "urls                     1292 non-null object\n",
      "media_type               3404 non-null object\n",
      "media1                   3404 non-null object\n",
      "media2                   1183 non-null object\n",
      "media3                   805 non-null object\n",
      "media4                   403 non-null object\n",
      "trade_topic              5657 non-null int64\n",
      "china_topic              5657 non-null int64\n",
      "financial_topic          5657 non-null int64\n",
      "labor_topic              5657 non-null int64\n",
      "industry_topic           5657 non-null int64\n",
      "currency_rates_topic     5657 non-null int64\n",
      "general_related_topic    5657 non-null int64\n",
      "budget_topic             5657 non-null int64\n",
      "foreign_policy_topic     5657 non-null int64\n",
      "covid_topic              5657 non-null int64\n",
      "sensitive_topic          5657 non-null int64\n",
      "environment_topic        5657 non-null int64\n",
      "sentiment                5657 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(3), int64(17), object(16)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('str_id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('vp_tweets_reducted_after_nlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
